{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6801651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd \n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configuring logging to capture any errors or info\n",
    "logging.basicConfig(filename=\"screener.log\",level=logging.DEBUG)\n",
    "\n",
    "# Credentials for login\n",
    "username=\"coyew87802@jzexport.com\"\n",
    "password=\"Coye@14789\"\n",
    "\n",
    "# Setting up Chrome WebDriver\n",
    "ser=Service(\"C:/Users/Sahil Bendre/Downloads/chromedriver-win64/chromedriver.exe\")\n",
    "driver=webdriver.Chrome(service=ser)\n",
    "\n",
    "# Opening the website\n",
    "driver.get(\"https://www.screener.in/login/?\")\n",
    "try:\n",
    "    # Wait for page to load\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Finding and inputting username\n",
    "    user_input=driver.find_element(\"xpath\",\"/html/body/main/div[2]/div[2]/form/div[1]/input\")\n",
    "    for i in username:\n",
    "        time.sleep(0.5)\n",
    "        user_input.send_keys(i)\n",
    "        time.sleep(1)\n",
    "    logging.info(\"Usename Added\")\n",
    "    \n",
    "    # Finding and inputting password\n",
    "    pass_input=driver.find_element(\"xpath\",\"/html/body/main/div[2]/div[2]/form/div[2]/input\")\n",
    "    for i in password:\n",
    "        time.sleep(1)\n",
    "        pass_input.send_keys(i)\n",
    "        time.sleep(1)\n",
    "    pass_input.send_keys(Keys.ENTER)\n",
    "    logging.info(\"password Added\")\n",
    "    \n",
    "    # Clicking on a link after login\n",
    "    driver.find_element(\"xpath\",\"/html/body/div/div[2]/aside/ul/li[2]/a\").click()\n",
    "    logging.info(\"Link open\")\n",
    "    \n",
    "    # Scraping data from the page\n",
    "    parent_div=driver.find_element(\"xpath\",\"/html/body/div/div[2]/main/div[2]\")\n",
    "\n",
    "    company_detail={}\n",
    "    a=[]\n",
    "    # Scraping company names and details\n",
    "    company_names=parent_div.find_elements(\"css selector\",\"[class='hover-link ink-900']\")\n",
    "    company=parent_div.find_elements(\"css selector\",\"[class='flex-row flex-space-between flex-align-center margin-top-32 margin-bottom-16 margin-left-4 margin-right-4']\")\n",
    "    \n",
    "    c=[]\n",
    "    for i in company:\n",
    "        span_text = i.text\n",
    "        c.append(span_text)\n",
    "      \n",
    "    nested_list = []\n",
    "    # Splitting and formatting the details\n",
    "    for item in c:\n",
    "        sublist = item.split('\\n')  \n",
    "        nested_list.append(sublist)\n",
    "        \n",
    "    company_name=[]\n",
    "    for i in range(len(nested_list)):\n",
    "        names=nested_list[i][0]\n",
    "        company_name.append(names)\n",
    "    nested_list = [sublist[1:] for sublist in nested_list] \n",
    "\n",
    "    for i in range(len(nested_list)):\n",
    "        if not nested_list[i]:\n",
    "            nested_list[i] = \"null\"\n",
    "   \n",
    "    # Creating dictionary with company names and details\n",
    "    company_detail['Company Names']=company_name\n",
    "    company_detail['Company Detail']=nested_list\n",
    "\n",
    "    dict_length = len(company_detail)\n",
    "\n",
    "    df1 = pd.DataFrame(company_detail)\n",
    "    df1.to_csv('output.csv', index=False)    \n",
    "except Exception as e:\n",
    "    # Logging any exceptions\n",
    "    logging.info(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e80321",
   "metadata": {},
   "source": [
    "Approach:\n",
    "\n",
    "-The script uses Selenium for web automation.\n",
    "-It first opens the Screener website, inputs login credentials, and navigates to a specific page.\n",
    "-It then scrapes data from the page, specifically company names and details.\n",
    "-Finally, it saves the scraped data into a CSV file.\n",
    "\n",
    "Challenges Faced:\n",
    "\n",
    "-Identifying correct XPaths and CSS selectors to locate elements on the webpage accurately.\n",
    "-Parsing and formatting the scraped data correctly to maintain the integrity of the information.\n",
    "\n",
    "Overcoming Challenges:\n",
    "\n",
    "-Ensuring proper waits using time.sleep() to handle dynamic loading and timing issues.\n",
    "-Using try-except blocks to catch and log any exceptions that may occur during script execution, helping in debugging and troubleshooting.\n",
    "-Utilizing logging to capture additional information about script execution, such as successful steps or encountered errors, aiding in understanding and resolving issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35ccf143",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43de4f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7bb30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
